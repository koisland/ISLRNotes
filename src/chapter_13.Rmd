---
title: "Chapter 13: Multiple Testing"
output:
  html_document:
    toc: true
    toc_depth: 4
    theme: united
    toc_float: true
---

```{r include=FALSE}
library(tidyverse)
```

The code below describes chapter 13 of *An Introduction to statistical learning with applications in R*, *Multiple Testing*.

## 13.1: A Quick Review of Hypothesis Testing

Hypothesis testing provides a rigorous statistical framework for answering "yes-or-no" questions about data.

-   Ex. Is there a difference in the mean blood pressure of lab mice in the control vs. the treatment group?

### 13.1.1 Testing a Hypothesis

1.  Define null and alternative hypotheses.
2.  Construct a test statistic that summarizes the strength of evidence *against the null hypothesis*.
3.  Compute a p-value that quantifies the probability of having obtained *a comparable or more extreme value of the test statistic under the null hypothesis.*
4.  Based on the p-value, we decide to reject the null hypothesis.

#### Define the Null and Alternate Hypotheses

Two possibilities:

-   Null ($H_o$)
    -   The default state of belief about the world.
-   Alternate ($H_a$)
    -   Something different and unexpected
    -   Suggests that the null hypothesis does not hold.

#### Construct the Test Statistic

A test statistic summaries the extent to which our data are consistent with $H_o$.

**Example: Mice Blood Pressure**

-   $n_t$ treatment mice
-   $n_c$ control mice
-   $x$ denotes the blood pressure measurements.
    -   Treatment: $x_1^t, ... x^t_{n_t}$
    -   Control: $x_1^c, ... x^c_{n_c}$

Calculate a **two-sample t-statistic** to test $H_o$ where the average blood pressure of both groups are equivalent.

<center>$T = \frac{\hat{\mu}_t - \hat{\mu}_c}{s \sqrt{\frac{1}{n_t} + \frac{1}{n_c}}}$</center>

-   $\hat{\mu}_t/\hat{\mu}_c$\$ Are the averages of the blood pressure measurements for both groups.
-   $s$ is an estimator of the pooled standarad deviation of the two sample groups.

> A large (absolute) value of $T$ provides evidence against the null hypothesis.

#### Compute the p-value

Next we need to determine how much evidence against $H_o$ is provided by a large test statistic.

> A p-value is the probability of observing a test statistic equal to or more extreme than than the observed statistic under the assumption that the $H_o$ is true.

-   A small p-value provides evidence against $H_o$.

**Example:** $T = 2.33$

-   Under $H_o$, distribution follows $N(0,1)$ distribution.
    -   A normal distribution with mean of 0 and variance of 1.
    -   Majority (98%) of $T$ values fall between $-2.33$ and $2.33$.
    -   Expect to see our result only 2% of the time.

Null distribution depends on:

-   Type of $H_o$ being tested.
-   Type of test statistic is used.

Most commonly-used test stats follow a well-known statistical distribution under $H_o$.

#### Interpretation

Often incorrectly interpreted. The CORRECT interpretation is:

> Given that the $H_o$ is true, the p-value is the chance we'd see this value of the test statistic or something more extreme if the test was repeated many times.

Converting the test statistic (An arbitrary and uninterpretable number) into a **p-value** (an easy-to-understand value between 0 and 1) **helps researchers interpret a result**.

-   Additional context given by p-value as we now what should be expected under $H_o$ null distribution.

#### Decide Whether to Reject the Null Hypothesis

Typically, some arbitrary threshold of $<0.05$.

Should never blindly adhere to a value. Also, include p-value in reporting.

### 13.1.2 Type I and Type II Errors

If the $H_o$ holds, it is a true null hypothesis. Otherwise it's a false null hypothesis.

| Decision            | Outcome if $H_o$ is true | Outcome if $H_a$ is true |
|---------------------|--------------------------|--------------------------|
| Reject $H_o$        | **Type I Error**         | Correct                  |
| Do Not Reject $H_o$ | Correct                  | **Type II Error**        |

#### Type 1 Error

-   Rate of error is the probability of incorrectly rejecting $H_o$.
-   Also called FPR.

> Considered worst type of error as declare finding that is not correct.

#### Type 2 Error

-   Rate of error is the probability of incorrectly not rejecting $H_o$.
-   Also called FNR.

> Its **inverse (1 - FNR)** is defined as the **power of a hypothesis test**. Correctly reject $H_o$ given $H_a$ holds.

#### Trade-Offs

-   Make Type I error small:
    -   By:
        -   Reject $H_o$ if we are quite sure that it doesn't hold.
    -   Downside:
        -   Increase in Type II error.
-   Make Type II error small:
    -   By:
        -   Reject $H_o$ with even modest amount of evidence.
    -   Downside:
        -   Increase in Type I error.

There is a direct correspondence between the p-value threshold that causes us to reject $H_o$ and the Type I error rate.

> By only rejecting $H_o$ when the p-value is below $\alpha$, ensure that Type I error rate will be less than or equal to $\alpha$.

## 13.2 the Challenge of Multiple Testing

<center>![](https://www.explainxkcd.com/wiki/images/3/3f/significant.png){height="33%"}</center>

How do we reject all null hypotheses for which the corresponding p-value falls below some threshold?

**Example: Flip Fair Coins**

-   $n = 1024$ coins.
-   $x = 10$ flips per coin.
-   What is the change that any single coin comes up *all tails*?
    -   $\frac{1}{2 ^ {10}} = \frac{1}{1,024} \approx 0.001$
    -   Standard hypothesis test for the $H_o$ (**This particular coin is fair.**) gives a p-value below **0.002**.
        -   Report both possibilities: 10 tails or 10 heads.

> When testing a large number of null hypotheses, we are bound to get some very small p-values by chance.

Another example. If FPR or $\alpha = 0.01$:

-   Test $m$ null hypotheses where $m = 10,000$.
-   $NumberNullHypothesesRejected = 0.01 * m = 100$
-   100 $H_o$ rejected by chance!

## 13.3 The Family-Wise Error Rate

### 13.3.1 What is the Family-Wise Error Rate?

Also called FWER.

-   When setting of $m$ null hypotheses.
-   Probability of making at least one Type I error among $m$ $H_o$s.

Possibilities when performing $m$ hypothesis tests.

| Action              | $H_o$ is True | $H_o$ is False | Total |
|---------------------|---------------|----------------|-------|
| Reject $H_o$        | $V$           | $S$            | $R$   |
| Do Not Reject $H_o$ | $U$           | $W$            | $m-R$ |

-   $V$ = Number of Type I Errors
-   $S$ = Number of true positives
-   $U$ = Number of true negatives
-   $W$ = Number of Type II Errors

Family wise error rate is given by:

<center>$FWER = Pr(V >= 1)$</center>

When accounting for level $\alpha$, FPR results in the following FWER.

<center>

$FWER(\alpha) = 1-Pr(V=0)$

-   $Pr(V=0)$ is the probability we do not falsely reject **any** null hypotheses.
-   Also, $Pr(\cap^m_{j=1}\{$do not falsely reject $H_oj$ $\})$

</center>

Each test of the $m$ tests are independent therefore:

-   $FWER(\alpha) = 1 - (1-\alpha)^m$
-   At an $\alpha = 0.05$, the $FWER = 1 - (1-0.05)^{100} = 0.994$
    -   **A 99.4% chance of at least one FP!**

> Only for very small like $\alpha = 0.001$, can we ensure a small FWER for a moderate number of hypothesis tests. More evidence in reject a $H_o$ is required to control FWER than FPR for a single test.

### 13.3.2 Approaches to Control the Family-Wise Error Rate

Using the `Fund` data set, we'll test general-purpose as well as more specialized methods for controlling the FWER.

-   Records the monthy percentage excess returns for 2,000 fund managers.
-   $n=50$ months

| Manager | Mean, $\overline{x}$ | Standard Deviation, $s$ | t-statistic | p-value |
|---------------|---------------|---------------|---------------|---------------|
| One     | 3.0                  | 7.4                     | 2.86        | 0.006   |
| Two     | -0.1                 | 6.9                     | -0.10       | 0.918   |
| Three   | 2.8                  | 7.5                     | 2.62        | 0.012   |
| Four    | 0.5                  | 6.7                     | 0.53        | 0.601   |
| Five    | 0.3                  | 6.8                     | 0.31        | 0.756   |

#### The Bonferroni Method

Event of Type I Error ($A_j$) for $jth$ null hypothesis among $m$ null hypotheses.

-   $j = 1, ..., m$
-   $FWER(\alpha) = Pr($falsely reject **at least** one null hypothesis$)$
    -   $\le \sum_{j=1}^{m} Pr(A_j)$
    -   $Pr(A \cup B) \le Pr(A) + Pr(B)$ (regardless if independent)

With the Bonferroni method:

-   Set the threshold for rejecting each hypothesis test to $\alpha / m$ so that $Pr(A_j) \le \alpha / m$ controlling the FPR at $\alpha$.

> If $\alpha = 0.1$ for $m=100$ null hypotheses, each $H_o$ can only be rejected if its p-value is below $0.001 = 0.1 / 100$

For the example above:

-   $H_o$ = The mean return of a manager is 0.
-   $H_a$ = The mean return of a manager is greater than 0.
-   Without Bonferroni, reject $H_{oj}$ for 1st and 3rd manager incorrectly.
    -   $FWER \ge 0.05$
-   With Bonferroni:
    -   $\alpha/m = 0.05 / 5 = 0.01$
    -   Only reject $H_{o1}$.

##### Downside

Very conservative approach to FWER control. Reject fewer null hypotheses and therefore increase Type II errors.

#### Holm's Step-Down Procedure {#holm}

Alternative to Bonferonni method:

-   a.k.a. Holm-Bonferroni method
-   Less conservative than Bonferroni method.
-   Makes no independence assumptions about the $m$ hypothesis tests.s

>  Reject more $H_o$ which decreases FNR (Type II Errors) and increases power. Preferred over Bonferroni.

##### Algorithm

1. Specify $\alpha$ level to control FWER.

2. Compute p-values for the $m$ null hypotheses.

3. Sort the p-values in ascending order.

4. Define $L$, the index of the threshold p-value.

<center>
$L = min \{ j: p_{(j)} > \frac{\alpha}{m + 1 -j} \}$
</center>

5. Reject all null hypotheses $H_{0j}$ for which $p_j < P_{(L)}$

For the example above:
```{r}
p_vals <- c(
  "1" = 0.006,
  "2" = 0.918,
  "3" = 0.012,
  "4" = 0.601,
  "5" = 0.756
)
```

```{r}
# Check if p-value is valid for Holm-Bonferonni procedure for controlling FWER.
# @param `p_val` - (numeric) p-value
# @param `m`     - (numeric) Number of null hypotheses
# @param `j`     - (numeric) p-value position among `m`
# @param `alpha` - (numeric) Level
#
# @return        - (bool) Reject null?
holms_method <- function(p_val, m, j, alpha = 0.05) {
  return(p_val < (alpha / (m + 1 - j)))
}
```

```{r}
for (i in order(p_vals)) {
  res <- holms_method(p_vals[i], length(p_vals), i, alpha = 0.05)
  switch(
    # R coerces keys to numeric
    # Can't use numbers for switch statements unless positional cases.
    toString(i),
    # Only 1st and 3rd manager are rejected.
    "1" = stopifnot(isTRUE(res)),
    "3" = stopifnot(isTRUE(res)),
    stopifnot(!isTRUE(res))
  )
}
```

Here, Holm's method is more powerful than the Bonferroni method as it rejects $H_{o1}$ and $H_{o3}$.

#### Two Special Case: Tukey's Method and Scheffe's Method
Can be more powerful than Bonferroni and Holm's methods in controlling FWER in certain tasks.

For `Fund` dataset:

- Manager One and Two have the greatest difference in return means.
- $H_o$: The returns of the two managers is equal.
  - Two-sample t-test for $H_o$ yields p-value of 0.0349. Some evidence to reject.
  - Misleading as only compared average returns of M1 and M2 **after looking at returns of all 5 managers.**
    - Performed $m = 5 * (5 - 1) / 2 = 10$ hypothesis tests and selected one with smallest p-value.
    - Could correct with Bonferroni and only reject if below $0.05 / 10 = 0.005$.
      - Not optimal as too stringent. **p-values for the comparisons are related so not independent.**

##### Tukey's method

- When performing $m=G(G-1)/2$ pairwise comparisons of $G$ means, the method controls the FWER at level $\alpha$ while rejecting all $H_o$s where p-value falls below $\alpha_T$ for some $\alpha_T > \alpha / m$.

##### Scheffe's method

From `Fund` data, notice that M1 and M3 have higher mean returns than M2, M4, and M5.

- Testing $H_o$, that the average returns of two groups of managers are equal:
  - $H_o = \frac{1}{2}(\mu_1 + \mu_3) = \frac{1}{3}(\mu_2 + \mu_4 + \mu_5)$
- Can use variant of two-sample t-test. Get p-value of 0.004 but **cannot reject $H_o$**.
  - Similar to above where seeing data means we essentially performed multiple hypothesis testing.

Using Scheffe's method, we can control FPR error at level $\alpha$ by computing a value $\alpha_S$ that can be used to reject $H_o$ if p-value is below $\alpha_s$.

- Can also use the same $\alpha_S$ for other combination of the same number of groupings.
  - ex. (M1 + M4) vs. (M2 + M3 + M5)

### 13.3.3 Trade-Off Between the FWER and Power

As the number of $m$ null hypotheses increases, power decreases at a given FWER.

- Remember, FWER at a given level $\alpha$ means that we are unlikely ($\le\alpha$) to reject **any true** $H_o$s (FPR)
- At large $m$, this limits rejection to very few $H_o$ to guarantee above statement.

## 13.4 The False Discovery Rate

### 13.4.1 Intuition for the False Discovery Rate
Using FWER to prevent FPs is too stringent when $m$ is large.

An alternative might be the ratio of FPs ($V$) to total Ps ($V + S = R$).

- This is the false discovery proportion (FDP).
- Should be low.
- But, ratio cannot be controlled as no way to be certain which hypotheses are true and which are false.
  - Similar to FWER.

Instead, control the **false discovery rate (FDR)**.

<center>
$FDR = E(FDP) = E(V/R)$
</center>

- FDR at level $q = 20\%$
  - Rejects as many $H_o$s as possible.
  - Guarantee that no more than $20\%$ of these are FPs, on average.

#### Applications

> For exploratory, rather than confirmatory purposes.

- Example:
  - Genomic researcher will sequence the genomes of individuals w/ or w/o a medication condition and for each of 20,000 genes, test whether sequence variants in that gene are associated with the medical condition of interest.
  - **Essentially performing $m = 20,000$ hypothesis tests.**
  - Goal is to see whether there is **an association between a gene and a disease** with further plans to investigate any genes for which there is evidence.
  - FWER not appropriate as we don't mind if a few FPs.
  - Need correction as cannot investigate all genes with p-values less than 0.05 (1000 would be considered significant by chance.)
    - With FDR, no more than 20% of genes studied are FPS.

> No standard accepted threshold for FDR control; it's context/data dependent.

### 13.4.2 The Benjamini-Hochberg Procedure
Procedure to reject $H_o$s while controlling $FDR \le q$

- Connect p-values from $m$ null hypotheses to desired FDR value, $q$.

##### Algorithm

1. Specify $q$ level to control FDR.

2. Compute p-values for the $m$ null hypotheses.

3. Sort the p-values in ascending order.

4. Define index $L = max \{ j : p_{(j)} < qj / m\}$.

5. Reject all null hypotheses $H_{oj}$ for which $p_j \le p_{(L)}$

```{r}
# Check if p-value is valid for Benjamini-Hochberg procedure for controlling FDR
# @param `p_val` - (numeric) p-value
# @param `m`     - (numeric) Number of null hypotheses
# @param `j`     - (numeric) p-value position among `m`
# @param `alpha` - (numeric) Level
#
# @return        - (bool) Reject null?
benjamini_hochberg_procedure <- function(p_val, m, j, alpha = 0.05) {
  return(p_val < (alpha * (j / m)))
}
```

```{r}
for (i in order(p_vals)) {
  res <- benjamini_hochberg_procedure(
    p_vals[i],
    length(p_vals),
    i,
    alpha = 0.05
  )
  switch(toString(i),
    # Only 1st and 3rd manager are rejected.
    "1" = stopifnot(isTRUE(res)),
    "3" = stopifnot(isTRUE(res)),
    stopifnot(!isTRUE(res))
  )
}
```

#### Summary
As long as $m$ p-values are independent, BH ensures that $FDR \le q$.

> On average, no more than a fraction $q$ fo the rejected $H_o$ are FPs.

  - Doesn't matter how many $H_o$ are true.
  - Doesn't matter what distribution of p-values for $H_o$ that are false.
  - Easy way to determine which of $m$ null hypotheses to reject in order to control FDR at $q$.
  - Data dependent; need to see data first to set threshold.
    - Similar to [Holm](#holm) procedure


## 13.5 A Re-Sampling Approach to p-Values and False Discovery Rates

Typically, we're intersted in a $H_o$ using a test statistic $T$ which has a distribution under $H_o$.

  - Called a theoretical null distribution.
    - ex. normal distribution, t-distribution, $\chi^2$-distribution, etc.
    - For most $H_o$, a null distribution is available.

However, may be unavailable if $H_o$ or test statistic $T$ is unusual.

- May also need to avoid because an assumption is false or sample size is small.
- In this scenario, need framework for performing inference.
  - Takes advantage of fast computers to approximate null distribution of $T$ and obtain a p-value.


### 13.5.1 A Re-Sampling Approach to the p-Value

Want to test whether the mean of a random variable $X$ equals the mean of a random variable $Y$.

Given $n_X$ independent observations from $X$ and $n_Y$ independent observations from $Y$, two-sample t-statistic is:

<center>
$T = \frac{\hat\mu_X - \hat\mu_Y}{s \sqrt{\frac{1}{n_X} + \frac{1}{n_Y}}}$
</center>

> If $n_X$ and $n_Y$ are large, then $T$ appoximately follows N(0,1) distribution.
> If $n_X$ and $n_Y$ are small, w/o strong assumption about distribution of $X$ and $Y$, don't known null distribution of $T$.

Can approximate the null distribution of $T$ using a re-sampling approach or a permutation approach.

> TODO: Skipping for now.

## 13.6 Lab: Multiple Testing

### 13.6.1 Review of Hypothesis Tests
Initialize some data with:

- 100 variables (cols) each with 10 observations (rows).

```{r}
set.seed(6)
num_vars <- 100
num_observations <- 10

x <- matrix(
  rnorm(num_observations * num_vars),
  nrow = num_observations,
  ncol = num_vars
)
```

Modify the first 50 variables.

- First 50 variables, have mean 0.5 and variance 1.
- The rest have mean 0 and variance 1.

```{r}
mean_adj <- 0.5
x[, 1:50] <- x[, 1:50] + mean_adj
```

Use `t.test()` to perform one-sample or two-sample t-test.

1. $H_o: \mu_1 = 0$
    - First variable has mean zero.

```{r}
# First variable (col)
t.test(x[, 1], mu = 0)
```

Fail to reject $H_o$ at level $\alpha = 0.05$.

- $\mu_1 = 0.5$ so $H_o$ is false.
- This is a FN (Type II error) as we fail to reject $H_o$ when it is false.

Test $H_{0j}: \mu_j = 0$ for $j = 1,...100$
```{r}
mu <- 0.05
p_vals <- rep(0, num_vars)

for (i in 1:num_vars) {
  t_test_res <- t.test(x[, i], mu = 0)
  p_vals[i] <- t_test_res$p.value
}

decision <- rep("Do not reject H0", num_vars)
decision[p_vals <= mu] <- "Reject H0"
```

Construct table to better visualize:
```{r}
table(
  decision,
  c(rep("H0 is False", 50), rep("H0 is True", 50))
)
```

- FN = 40
    - Simulation has a weak signal as ratio of mean to stdev is small $(0.5 / 1.0) = 0.5$
- FP = 3

### 13.6.2 The Family-Wise Error Rate
```{r}
m <- 1:500
# Calculated FWER
# * num_htests - m, number of hypothesis tests.
# * alpha
fwe <- function(alpha, num_htests) {
  1 - (1 - alpha)^num_htests
}
```

Calculate FWER for multiple alpha levels:
* Shows that unless $m$ is large or $\alpha$ is small, FWER exceeds 0.05.
* Lowering $\alpha$ increases FNs. Low power.

```{r}
fwe1 <- fwe(0.05, m)
fwe2 <- fwe(0.01, m)
fwe3 <- fwe(0.001, m)
```

```{r}
par(mfrow = c(1, 1))
plot(
  m,
  fwe1,
  type = "l",
  log = "x",
  ylim = c(0, 1),
  col = 2,
  ylab = "Family - Wise Error Rate",
  xlab = "Number of Hypotheses"
)
title("Number of Hypotheses on FWER")
lines(m, fwe2, col = 4)
lines(m, fwe3, col = 3)
abline(h = 0.05, lty = 2)
legend(
  "topleft",
  legend = c(0.05, 0.01, 0.001),
  col = c(2, 4, 3),
  pch = rep("*", 3),
  title = "Alphas"
)
```

Using `Fund` dataset, test $H_o$ that $jth$ fund manager's mean return is zero.
```{r}
# Load dataset
library(ISLR2)
fund_mini <- Fund[, 1:5]
# Run one-sample t-test
t.test(fund_mini[, 1], mu = 0)

# Store values in numeric array.
fund_pvalues <- rep(0, 5)
for (i in 1:5) {
  fund_pvalues[i] <- t.test(fund_mini[, i], mu = 0)$p.value
}
print("P-values before Error Correction Adjustment")
fund_pvalues
```

```{r include=FALSE}
print_reject_managers <- function(p_values, alpha = 0.05) {
  sprintf(
    "We can reject Manager %d, at alpha of %f",
    which(p_values < alpha),
    alpha
  )
}
```
Adjust for multiple hypothesis testing, using `p.adjust()`.
```{r}
fund_pvalues_bonf <- p.adjust(fund_pvalues, method = "bonferroni")
fund_pvalues_bonf
# Wrapper on which and sprintf
print_reject_managers(fund_pvalues_bonf)
```
```{r}
# Under the hood, p.adjust does this. pmin calculates min for multiple arrays.
# The 1 arg is the max value allowed.
pmin(fund_pvalues * 5, 1)
```
> With our new p-values, if value for a given test is $\le \alpha$, we can reject it while maintaining a FWER of no more than $\alpha$.

```{r}
fund_pvalues_holm <- p.adjust(fund_pvalues, method = "holm")
fund_pvalues_holm
print_reject_managers(fund_pvalues_holm)
```

Manager One does well whereas Manager Two does not. Is there a difference in performance between the two?

- Appears like there is a significant difference between the two.

```{r}
apply(fund_mini, 2, mean)
```

```{r}
paired_ttest_manager1_2 <- t.test(
  fund_mini[, 1],
  fund_mini[, 2],
  paired = TRUE
)
paired_ttest_manager1_2
```

But, because we looked at data before running test, performed 10 hypothesis tests. Need to use Tukey's method (`TukeyHSD()`) to adjust for multiple testing.

- Requires output of ANOVA regression model
    - Response: Monthly excess returns achieved by each manager
    - Predictor: Manager which each return corresponds.
- No longer significant difference.

```{r}
returns <- as.vector(as.matrix(fund_mini))
managers <- rep(
  # managers
  c("1", "2", "3", "4", "5"),
  rep(50, 5)
)
# ANOVA regression model.
a1 <- aov(returns ~ managers)

tukey <- TukeyHSD(x = a1)
tukey
```
```{r}
plot(tukey)
```

### 13.6.3 The False Discovery Rate
Perform hypothesis tests (one-sample t-test) for all 2,000 fund managers in dataset.

- Too many managers to try and control the FWER so control FDR instead.
```{r}
num_fund_managers <- length(Fund)
fund_pvalues_fdr <- rep(0, num_fund_managers)
for (i in 1:num_fund_managers) {
  fund_pvalues_fdr[i] <- t.test(Fund[, i], mu = 0)$p.value
}
```


Q-values are output and are the smallest FDR threshold at which we could reject a particular $H_o$.

- ex. $q = 0.1$
    - Reject corresponding $H_o$ at an FDR of 10% or greater. Cannot reject the $H_o$ at an FDR below 10%.

```{r}
q_values_bh <- p.adjust(fund_pvalues_fdr, method = "BH")
q_values_bh[1:10]
```

```{r}
q_value <- 0.1
sum(q_values_bh <= q_value)
```

The number of fund managers with q-value below 0.1.

> 146 of fund managers beat the market at an FDR of 10%. Only 15 (10% of 146) are likely to be false discoveries.

```{r}
sum(fund_pvalues <= (0.1 / 2000))
```

> With Bonferroni's method, no $H_o$ is rejected.

Reimplemented in book.

```{r}
sorted_pvals <- sort(fund_pvalues)
m <- length(fund_pvalues)
q <- 0.1
check_pvals_idxs <- which(sorted_pvals < q * (1:m) / m)
# Index finding index to reject other p-vals.
if (length(check_pvals_idxs) > 0) {
  rej_pvals_idxs <- 1:max(check_pvals_idxs)
} else {
  rej_pvals_idxs <- numeric(0)
}
```

```{r}
plot(
  sorted_pvals,
  log = "xy",
  ylim = c(4e-6, 1),
  ylab = "P-Value",
  xlab = "Index",
  main = ""
)
points(rej_pvals_idxs, sorted_pvals[rej_pvals_idxs], col = 4)
abline(a = 0, b = (q / m), col = 2, untf = TRUE)
abline(h = 0.1 / 2000, col = 3)
```

## 13.7 Exercises
1.
2.
3.
4.
5.
6.
7.
