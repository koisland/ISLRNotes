---
title: "Chapter 13: Multiple Testing"
output:
  html_document:
    toc: true
    toc_depth: 4
    theme: united
    toc_float: true
---

```{r include=FALSE}
library(tidyverse)
```

The code below describes chapter 13 of *An Introduction to statistical learning with applications in R*, *Multiple Testing*.

## 13.1: A Quick Review of Hypothesis Testing

Hypothesis testing provides a rigorous statistical framework for answering "yes-or-no" questions about data.

-   Ex. Is there a difference in the mean blood pressure of lab mice in the control vs. the treatment group?

### 13.1.1 Testing a Hypothesis

1.  Define null and alternative hypotheses.
2.  Construct a test statistic that summarizes the strength of evidence *against the null hypothesis*.
3.  Compute a p-value that quantifies the probability of having obtained *a comparable or more extreme value of the test statistic under the null hypothesis.*
4.  Based on the p-value, we decide to reject the null hypothesis.

#### Define the Null and Alternate Hypotheses

Two possibilities:

-   Null ($H_o$)
    -   The default state of belief about the world.
-   Alternate ($H_a$)
    -   Something different and unexpected
    -   Suggests that the null hypothesis does not hold.

#### Construct the Test Statistic

A test statistic summaries the extent to which our data are consistent with $H_o$.

**Example: Mice Blood Pressure**

-   $n_t$ treatment mice
-   $n_c$ control mice
-   $x$ denotes the blood pressure measurements.
    -   Treatment: $x_1^t, ... x^t_{n_t}$
    -   Control: $x_1^c, ... x^c_{n_c}$

Calculate a **two-sample t-statistic** to test $H_o$ where the average blood pressure of both groups are equivalent.

<center>$T = \frac{\hat{\mu}_t - \hat{\mu}_c}{s \sqrt{\frac{1}{n_t} + \frac{1}{n_c}}}$</center>

-   $\hat{\mu}_t/\hat{\mu}_c$\$ Are the averages of the blood pressure measurements for both groups.
-   $s$ is an estimator of the pooled standarad deviation of the two sample groups.

> A large (absolute) value of $T$ provides evidence against the null hypothesis.

#### Compute the p-value

Next we need to determine how much evidence against $H_o$ is provided by a large test statistic.

> A p-value is the probability of observing a test statistic equal to or more extreme than than the observed statistic under the assumption that the $H_o$ is true.

-   A small p-value provides evidence against $H_o$.

**Example:** $T = 2.33$

-   Under $H_o$, distribution follows $N(0,1)$ distribution.
    -   A normal distribution with mean of 0 and variance of 1.
    -   Majority (98%) of $T$ values fall between $-2.33$ and $2.33$.
    -   Expect to see our result only 2% of the time.

Null distribution depends on:

-   Type of $H_o$ being tested.
-   Type of test statistic is used.

Most commonly-used test stats follow a well-known statistical distribution under $H_o$.

#### Interpretation

Often incorrectly interpreted. The CORRECT interpretation is:

> Given that the $H_o$ is true, the p-value is the chance we'd see this value of the test statistic or something more extreme if the test was repeated many times.

Converting the test statistic (An arbitrary and uninterpretable number) into a **p-value** (an easy-to-understand value between 0 and 1) **helps researchers interpret a result**.

-   Additional context given by p-value as we now what should be expected under $H_o$ null distribution.

#### Decide Whether to Reject the Null Hypothesis

Typically, some arbitrary threshold of $<0.05$.

Should never blindly adhere to a value. Also, include p-value in reporting.

### 13.1.2 Type I and Type II Errors

If the $H_o$ holds, it is a true null hypothesis. Otherwise it's a false null hypothesis.

| Decision            | Outcome if $H_o$ is true | Outcome if $H_a$ is true |
|---------------------|--------------------------|--------------------------|
| Reject $H_o$        | **Type I Error**         | Correct                  |
| Do Not Reject $H_o$ | Correct                  | **Type II Error**        |

#### Type 1 Error

-   Rate of error is the probability of incorrectly rejecting $H_o$.
-   Also called FPR.

> Considered worst type of error as declare finding that is not correct.

#### Type 2 Error

-   Rate of error is the probability of incorrectly not rejecting $H_o$.
-   Also called FNR.

> Its **inverse (1 - FNR)** is defined as the **power of a hypothesis test**. Correctly reject $H_o$ given $H_a$ holds.

#### Trade-Offs

-   Make Type I error small:
    -   By:
        -   Reject $H_o$ if we are quite sure that it doesn't hold.
    -   Downside:
        -   Increase in Type II error.
-   Make Type II error small:
    -   By:
        -   Reject $H_o$ with even modest amount of evidence.
    -   Downside:
        -   Increase in Type I error.

There is a direct correspondence between the p-value threshold that causes us to reject $H_o$ and the Type I error rate.

> By only rejecting $H_o$ when the p-value is below $\alpha$, ensure that Type I error rate will be less than or equal to $\alpha$.

## 13.2 the Challenge of Multiple Testing

<center>![](https://www.explainxkcd.com/wiki/images/3/3f/significant.png){height="33%"}</center>

How do we reject all null hypotheses for which the corresponding p-value falls below some threshold?

**Example: Flip Fair Coins**

-   $n = 1024$ coins.
-   $x = 10$ flips per coin.
-   What is the change that any single coin comes up *all tails*?
    -   $\frac{1}{2 ^ {10}} = \frac{1}{1,024} \approx 0.001$
    -   Standard hypothesis test for the $H_o$ (**This particular coin is fair.**) gives a p-value below **0.002**.
        -   Report both possibilities: 10 tails or 10 heads.

> When testing a large number of null hypotheses, we are bound to get some very small p-values by chance.

Another example. If FPR or $\alpha = 0.01$:

-   Test $m$ null hypotheses where $m = 10,000$.
-   $NumberNullHypothesesRejected = 0.01 * m = 100$
-   100 $H_o$ rejected by chance!

## 13.3 The Family-Wise Error Rate

### 13.3.1 What is the Family-Wise Error Rate?

Also called FWER.

-   When setting of $m$ null hypotheses.
-   Probability of making at least one Type I error among $m$ $H_o$s.

Possibilities when performing $m$ hypothesis tests.

| Action              | $H_o$ is True | $H_o$ is False | Total |
|---------------------|---------------|----------------|-------|
| Reject $H_o$        | $V$           | $S$            | $R$   |
| Do Not Reject $H_o$ | $U$           | $W$            | $m-R$ |

-   $V$ = Number of Type I Errors
-   $S$ = Number of true positives
-   $U$ = Number of true negatives
-   $W$ = Number of Type II Errors

Family wise error rate is given by:

<center>$FWER = Pr(V >= 1)$</center>

When accounting for level $\alpha$, FPR results in the following FWER.

<center>

$FWER(\alpha) = 1-Pr(V=0)$

-   $Pr(V=0)$ is the probability we do not falsely reject **any** null hypotheses.
-   Also, $Pr(\cap^m_{j=1}\{$do not falsely reject $H_oj$ $\})$

</center>

Each test of the $m$ tests are independent therefore:

-   $FWER(\alpha) = 1 - (1-\alpha)^m$
-   At an $\alpha = 0.05$, the $FWER = 1 - (1-0.05)^{100} = 0.994$
    -   **A 99.4% chance of at least one FP!**

> Only for very small like $\alpha = 0.001$, can we ensure a small FWER for a moderate number of hypothesis tests. More evidence in reject a $H_o$ is required to control FWER than FPR for a single test.

### 13.3.2 Approaches to Control the Family-Wise Error Rate

Using the `Fund` data set, we'll test general-purpose as well as more specialized methods for controlling the FWER.

-   Records the monthy percentage excess returns for 2,000 fund managers.
-   $n=50$ months

| Manager | Mean, $\overline{x}$ | Standard Deviation, $s$ | t-statistic | p-value |
|---------------|---------------|---------------|---------------|---------------|
| One     | 3.0                  | 7.4                     | 2.86        | 0.006   |
| Two     | -0.1                 | 6.9                     | -0.10       | 0.918   |
| Three   | 2.8                  | 7.5                     | 2.62        | 0.012   |
| Four    | 0.5                  | 6.7                     | 0.53        | 0.601   |
| Five    | 0.3                  | 6.8                     | 0.31        | 0.756   |

#### The Bonferroni Method

Event of Type I Error ($A_j$) for $jth$ null hypothesis among $m$ null hypotheses.

-   $j = 1, ..., m$
-   $FWER(\alpha) = Pr($falsely reject **at least** one null hypothesis$)$
    -   $\le \sum_{j=1}^{m} Pr(A_j)$
    -   $Pr(A \cup B) \le Pr(A) + Pr(B)$ (regardless if independent)

With the Bonferroni method:

-   Set the threshold for rejecting each hypothesis test to $\alpha / m$ so that $Pr(A_j) \le \alpha / m$ controlling the FPR at $\alpha$.

> If $\alpha = 0.1$ for $m=100$ null hypotheses, each $H_o$ can only be rejected if its p-value is below $0.001 = 0.1 / 100$

For the example above:

-   $H_o$ = The mean return of a manager is 0.
-   $H_a$ = The mean return of a manager is greater than 0.
-   Without Bonferroni, reject $H_{oj}$ for 1st and 3rd manager incorrectly.
    -   $FWER \ge 0.05$
-   With Bonferroni:
    -   $\alpha/m = 0.05 / 5 = 0.01$
    -   Only reject $H_{o1}$.

##### Downside

Very conservative approach to FWER control. Reject fewer null hypotheses and therefore increase Type II errors.

#### Holm's Step-Down Procedure

Alternative to Bonferonni method:

-   a.k.a. Holm-Bonferroni method
-   Less conservative than Bonferroni method.
-   Makes no independence assumptions about the $m$ hypothesis tests.s

>  Reject more $H_o$ which decreases FNR (Type II Errors) and increases power. Preferred over Bonferroni.

##### Algorithm

1. Specify $\alpha$ level to control FWER.

2. Compute p-values for the $m$ null hypotheses.

3. Sort the p-values in ascending order.

4. Define $L$, the index of the threshold p-value.

<center>
$L = min \{ j: p_{(j)} > \frac{\alpha}{m + 1 -j} \}$
</center>

5. Reject all null hypotheses $H_{0j}$ for which $p_j < P_{(L)}$

For the example above:
```{r}
p_vals <- c(
  "1" = 0.006,
  "2" = 0.918,
  "3" = 0.012,
  "4" = 0.601,
  "5" = 0.756
)
```

```{r}
# Check if p-value is valid for Holm-Bonferonni procedure for controlling FWER.
# @param `p_val` - (numeric) p-value
# @param `m`     - (numeric) Number of null hypotheses
# @param `j`     - (numeric) p-value position among `m`
# @param `alpha` - (numeric) Level
#
# @return        - (bool) Reject null?
holms_method <- function(p_val, m, j, alpha = 0.05) {
  return(p_val < (alpha / (m + 1 - j)))
}
```

```{r}
for (i in order(p_vals)) {
  res <- holms_method(p_vals[i], length(p_vals), i, alpha = 0.05)
  switch(
    # R coerces keys to numeric
    # Can't use numbers for switch statements unless positional cases.
    toString(i),
    # Only 1st and 3rd manager are rejected.
    "1" = stopifnot(isTRUE(res)),
    "3" = stopifnot(isTRUE(res)),
    stopifnot(!isTRUE(res))
  )
}
```

Here, Holm's method is more powerful than the Bonferroni method as it rejects $H_{o1}$ and $H_{o3}$.

#### Two Special Case: Tukey's Method and Scheffe's Method
Can be more powerful than Bonferroni and Holm's methods in controlling FWER in certain tasks.

For `Fund` dataset:

- Manager One and Two have the greatest difference in return means.
- $H_o$: The returns of the two managers is equal.
  - Two-sample t-test for $H_o$ yields p-value of 0.0349. Some evidence to reject.
  - Misleading as only compared average returns of M1 and M2 **after looking at returns of all 5 managers.**
    - Performed $m = 5 * (5 - 1) / 2 = 10$ hypothesis tests and selected one with smallest p-value.
    - Could correct with Bonferroni and only reject if below $0.05 / 10 = 0.005$.
      - Not optimal as too stringent. **p-values for the comparisons are related so not independent.**

##### Tukey's method

- When performing $m=G(G-1)/2$ pairwise comparisons of $G$ means, the method controls the FWER at level $\alpha$ while rejecting all $H_o$s where p-value falls below $\alpha_T$ for some $\alpha_T > \alpha / m$.

##### Scheffe's method

From `Fund` data, notice that M1 and M3 have higher mean returns than M2, M4, and M5.

- Testing $H_o$, that the average returns of two groups of managers are equal:
  - $H_o = \frac{1}{2}(\mu_1 + \mu_3) = \frac{1}{3}(\mu_2 + \mu_4 + \mu_5)$
- Can use variant of two-sample t-test. Get p-value of 0.004 but **cannot reject $H_o$**.
  - Similar to above where seeing data means we essentially performed multiple hypothesis testing.

Using Scheffe's method, we can control FPR error at level $\alpha$ by computing a value $\alpha_S$ that can be used to reject $H_o$ if p-value is below $\alpha_s$.

- Can also use the same $\alpha_S$ for other combination of the same number of groupings.
  - ex. (M1 + M4) vs. (M2 + M3 + M5)

### 13.3.3 Trade-Off Between the FWER and Power

As the number of $m$ null hypotheses increases, power decreases at a given FWER.

- Remember, FWER at a given level $\alpha$ means that we are unlikely ($\le\alpha$) to reject **any true** $H_o$s (FPR)
- At large $m$, this limits rejection to very few $H_o$ to guarantee above statement.

## 13.4 The False Discovery Rate

### 13.4.1 Intuition for the False Discovery Rate
Using FWER to prevent FPs is too stringent when $m$ is large.

An alternative might be the ratio of FPs ($V$) to total Ps ($V + S = R$) should be low.

- This is the false discovery proportion (FDP).
- But, ratio cannot be controlled as no way to be certain which hypotheses are true and which are false.
  - Similar to FWER.

Instead, control the **false discovery rate (FDR)**.

<center>
$FDR = E(FDP) = E(V/R)$
</center>

- FDR at level $q = 20\%$
  - Rejects as many $H_o$s as possible.
  - Guarantee that no more than $20\%$ of these are FPs, on average.
